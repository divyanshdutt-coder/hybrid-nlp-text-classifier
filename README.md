# Hybrid Text Classification System for AI vs Human Content Detection

This project presents a production-grade hybrid text classification system designed to distinguish between human-written and AI-generated text in modern web applications. It combines explainable algorithmic intelligence with transformer-based deep learning models to deliver high accuracy, transparency, and real-time performance.

The algorithmic classification pipeline is built using advanced Natural Language Processing (NLP) and statistical analysis. It extracts seven core linguistic features, including lexical diversity, sentence length variance, n-gram repetition patterns, punctuation density, contraction usage, stopword distribution, and readability metrics (Flesch Reading Ease). These features are normalized, weighted, and aggregated using sigmoid-based scoring to generate an interpretable AI-likelihood score, enabling feature-level explainability and transparent decision-making.

In parallel, the system integrates a transformer-based deep learning model fine-tuned using the Hugging Face ecosystem. Trained on curated datasets of human-written and AI-generated text, the model leverages modern tokenization, contextual embeddings, and probabilistic inference to capture deeper semantic and stylistic patterns beyond rule-based detection.

The solution is deployed as a scalable full-stack web application. The backend is developed using Python and Flask, exposing RESTful APIs for text preprocessing, model inference, and hybrid score fusion, while the frontend is built with React, HTML5, CSS3, and JavaScript to provide a responsive, real-time user experience. The hybrid decision engine combines outputs from both pipelines to deliver model-wise confidence scores, explainability feedback, and reliable classification results for applications such as academic integrity verification, content authenticity analysis, and AI-generated text detection.
